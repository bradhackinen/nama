<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>nama.token_similarity API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>nama.token_similarity</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from itertools import combinations
from collections import Counter
import pandas as pd
import numpy as np
import regex as re
from tqdm import tqdm

from .matcher import Matcher
from .scoring import score_predicted


def ngrams(string, n=2):
    &#34;&#34;&#34;
    Generates all n-grams of the given string.

    Parameters
    ----------
    string: str
        The string from which the n-grams will be generated.
    n: int, optional
        The number of characters in each n-gram. Default is 2.

    Yields
    ------
    str
        An n-gram of the string.

    &#34;&#34;&#34;
    for i in range(len(string) - n + 1):
        yield string[i:i + n]


def nmgrams(string, n=1, m=3):
    &#34;&#34;&#34;
    Generates all n-grams of the given string where the n-grams are of length between n and m.

    Parameters
    ----------
    string: str
        The string from which the n-grams will be generated.
    n: int, optional
        The minimum length of the n-grams to be generated. Default is 1.
    m: int, optional
        The maximum length of the n-grams to be generated. Default is 3.

    Yields
    ------
    str
        An n-gram of the string where the length is between n and m.

    &#34;&#34;&#34;
    for j in range(n, m + 1):
        for i in range(len(string) - j + 1):
            yield string[i:i + j]


def words(string):
    &#34;&#34;&#34;
    Generates all words in the given string.

    Parameters
    ----------
    string: str
        The string from which the words will be extracted.

    Yields
    ------
    str
        A word extracted from the string.

    &#34;&#34;&#34;
    for m in re.finditer(r&#39;[A-Za-z0-9]+&#39;, string):
        yield m.group(0)


def jaccard_similarity(set0, set1, weights):
    &#34;&#34;&#34;
    Calculates the Jaccard similarity between two sets of terms.

    Parameters
    ----------
    set0: set
        The first set of terms.
    set1: set
        The second set of terms.
    weights: dict
        A dictionary mapping terms to weights.

    Returns
    -------
    float
        The Jaccard similarity between set0 and set1.

    &#34;&#34;&#34;
    intersection = set0 &amp; set1

    if not intersection:
        return 0

    union = set0 | set1
    denominator = sum(weights[t] for t in union)

    if not denominator:
        return 0

    numerator = sum(weights[t] for t in intersection)
    return numerator / denominator


def cosine_similarity(set0, set1, weights):
    &#34;&#34;&#34;
    Calculates the cosine similarity between two sets of terms.

    Parameters
    ----------
    set0: set
        The first set of terms.
    set1: set
        The second set of terms.
    weights: dict
        A dictionary mapping terms to weights.

    Returns
    -------
    float
        The cosine similarity between set0 and set1.

    &#34;&#34;&#34;
    intersection = set0 &amp; set1

    if not intersection:
        return 0

    length0 = np.sqrt(sum(weights[t]**2 for t in set0))
    length1 = np.sqrt(sum(weights[t]**2 for t in set1))

    denominator = length0 * length1
    if not denominator:
        return 0

    numerator = sum(weights[t]**2 for t in intersection)

    return numerator / denominator


class TokenSimilarity():
    class TokenSimilarity:
        &#34;&#34;&#34;
        Class that implements a token-based similarity model.
        &#34;&#34;&#34;

    def __init__(
            self,
            tokenizer=lambda s: nmgrams(
                s,
                2,
                3),
            weighting=&#39;tf-idf&#39;,
            measure=&#39;jaccard&#39;,
            max_block_size=100):
        &#34;&#34;&#34;
        Configures the token similarity model

        Parameters:
        -----------
        tokenizer : callable
            A function that takes a string an returns an iterable token generator (default: bigrams and trigrams)
        weighting : str
            One of: None,&#34;tf&#34;,&#34;idf&#34;, or &#34;&#34;tf-idf&#34;
        measure : str
            Either &#34;jaccard&#34; or &#34;cosine&#34;
        max_block_size : int
            Maximum size of blocks.
        &#34;&#34;&#34;

        self.tokenizer = tokenizer
        self.weighting = weighting
        self.measure = measure
        self.max_block_size = max_block_size

        # Assign weighting function
        if callable(weighting):
            self.weight_func = weighting
        elif weighting == &#39;tf&#39;:
            self.weight_func = lambda t, f, d: f
        elif weighting == &#39;idf&#39;:
            self.weight_func = lambda t, f, d: 1 / np.log(1 + d)
        elif weighting == &#39;tf-idf&#39;:
            self.weight_func = lambda t, f, d: f / np.log(1 + d)
        else:
            raise ValueError(&#39;Unknown weighting type&#39;)

        # Assign scoring function
        if callable(measure):
            self.score_func = measure
        elif measure == &#39;cosine&#39;:
            self.score_func = cosine_similarity
        elif measure == &#39;jaccard&#39;:
            self.score_func = jaccard_similarity
        else:
            raise ValueError(&#39;Unknown measure value&#39;)

        # Set threshold to None initially
        self.threshold = None

    def fit(self, matcher, learn_threshold=False,):
        &#34;&#34;&#34;
        Fits the token similarity model on the given matcher data.

        Parameters:
        -----------
        matcher : object
            The matcher object.
        learn_threshold : bool
            If True, learn the threshold.
        &#34;&#34;&#34;
        self.strings = matcher.strings()

        # Loops will run a little faster if we make local references to
        # functions
        tokenizer = self.tokenizer
        weight_func = self.weight_func

        # Tokenize strings
        tokenized = {s: list(tokenizer(s)) for s in self.strings}

        # Count occurrences of tokens
        counts = Counter(t for tokens in tokenized.values() for t in tokens)

        # Convert tokens to sets
        # (drops frequency information, and also makes membership tests faster)
        self.tokenized = {s: set(tokens) for s, tokens in tokenized.items()}

        # Count tokens again, this time tracking the number of strings
        # containing the token. In NLP this is usually referred to as the
        # &#34;document count&#34;
        doc_counts = Counter(t for tokens in tokenized.values()
                             for t in tokens)
        self.doc_counts = doc_counts

        # Build weights
        self.weights = {
            t: weight_func(
                t,
                f,
                doc_counts[t]) for t,
            f in counts.items()}

    def learn_threshold(
            self,
            gold_matcher,
            objective=&#39;F1&#39;,
            grid=np.linspace(
                0.5,
                1,
                100),
            use_counts=False):
        &#34;&#34;&#34;
        Uses train_matcher as a training set to choose the default similarity threshold.

        Parameters:
        -----------
        gold_matcher : object
            Gold matcher data.
        objective : str
            The evaluation objective.
        grid : ndarray
            Grid of values to evaluate.
        use_counts : bool
            If True, use counts.

        Returns:
        -------
        pandas.DataFrame
            The scores data frame.
        &#34;&#34;&#34;
        self_copy = TokenSimilarity(
            tokenizer=self.tokenizer,
            weighting=self.weighting,
            measure=self.measure,
            max_block_size=self.max_block_size)
        self_copy.fit(gold_matcher)

        scores = []
        for t in tqdm(grid):
            pred = self_copy.predict(threshold=t)
            s = score_predicted(pred, gold_matcher, use_counts=use_counts)
            s[&#39;threshold&#39;] = t
            scores.append(s)

        scores_df = pd.DataFrame(scores)

        self.threshold = scores_df[scores_df[objective] ==
                                   scores_df[objective].max()][&#39;threshold&#39;].values[-1]

        return scores_df

    def test(self, test_matcher):
        &#34;&#34;&#34;
        Evaluates the accuracy of the model using test_matcher as the gold-standard test set.

        Note: For a fair test, it may be important to ensure that the test and train matchers have no strings in common.

        Parameters:
        -----------
        test_matcher : object
            The test matcher data.

        Returns:
        -------
        dict
            The scores of the test.
        &#34;&#34;&#34;

        predicted = self.predict(test_matcher.strings())

        scores = scores = score_predicted(predicted, test_matcher)

        return scores

    def predict(self, strings=None, threshold=None):
        &#34;&#34;&#34;
        Uses the similarity model to predict matches between the passed strings.

        Parameters:
        -----------
        strings : list or None, optional
            The list of strings.
        threshold : float or None, optional
            The similarity threshold.

        Returns:
        -------
        dict
            The predicted matches.
        &#34;&#34;&#34;

        if strings:
            self.fit(strings)

        if threshold is None:
            if self.threshold is None:
                raise ValueError(
                    &#39;Must set a threshold value, either by calling .learn_threshold(), or by passing a manually chosen value as an argument&#39;)
            else:
                threshold = self.threshold

        strings = self.strings
        tokenized = self.tokenized
        weights = self.weights
        score_func = self.score_func

        predicted = Matcher(strings)

        # Iterate over unique tokens
        for t, d in self.doc_counts.items():
            # Find all strings that share this token (the &#34;block&#34;)
            if 2 &lt;= d &lt;= self.max_block_size:
                block = [s for s in strings if t in tokenized[s]]

                # Score all pairs in the block
                for s0, s1 in combinations(block, 2):

                    # Can skip pairs that are already in the same group
                    if predicted[s0] != predicted[s1]:

                        # Unite strings with score &gt;= threshold
                        score = score_func(
                            tokenized[s0], tokenized[s1], weights)
                        if score &gt;= threshold:
                            predicted.unite([s0, s1], inplace=True)

        return predicted</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="nama.token_similarity.cosine_similarity"><code class="name flex">
<span>def <span class="ident">cosine_similarity</span></span>(<span>set0, set1, weights)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the cosine similarity between two sets of terms.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>set0</code></strong> :&ensp;<code>set</code></dt>
<dd>The first set of terms.</dd>
<dt><strong><code>set1</code></strong> :&ensp;<code>set</code></dt>
<dd>The second set of terms.</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary mapping terms to weights.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The cosine similarity between set0 and set1.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cosine_similarity(set0, set1, weights):
    &#34;&#34;&#34;
    Calculates the cosine similarity between two sets of terms.

    Parameters
    ----------
    set0: set
        The first set of terms.
    set1: set
        The second set of terms.
    weights: dict
        A dictionary mapping terms to weights.

    Returns
    -------
    float
        The cosine similarity between set0 and set1.

    &#34;&#34;&#34;
    intersection = set0 &amp; set1

    if not intersection:
        return 0

    length0 = np.sqrt(sum(weights[t]**2 for t in set0))
    length1 = np.sqrt(sum(weights[t]**2 for t in set1))

    denominator = length0 * length1
    if not denominator:
        return 0

    numerator = sum(weights[t]**2 for t in intersection)

    return numerator / denominator</code></pre>
</details>
</dd>
<dt id="nama.token_similarity.jaccard_similarity"><code class="name flex">
<span>def <span class="ident">jaccard_similarity</span></span>(<span>set0, set1, weights)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the Jaccard similarity between two sets of terms.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>set0</code></strong> :&ensp;<code>set</code></dt>
<dd>The first set of terms.</dd>
<dt><strong><code>set1</code></strong> :&ensp;<code>set</code></dt>
<dd>The second set of terms.</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary mapping terms to weights.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The Jaccard similarity between set0 and set1.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jaccard_similarity(set0, set1, weights):
    &#34;&#34;&#34;
    Calculates the Jaccard similarity between two sets of terms.

    Parameters
    ----------
    set0: set
        The first set of terms.
    set1: set
        The second set of terms.
    weights: dict
        A dictionary mapping terms to weights.

    Returns
    -------
    float
        The Jaccard similarity between set0 and set1.

    &#34;&#34;&#34;
    intersection = set0 &amp; set1

    if not intersection:
        return 0

    union = set0 | set1
    denominator = sum(weights[t] for t in union)

    if not denominator:
        return 0

    numerator = sum(weights[t] for t in intersection)
    return numerator / denominator</code></pre>
</details>
</dd>
<dt id="nama.token_similarity.ngrams"><code class="name flex">
<span>def <span class="ident">ngrams</span></span>(<span>string, n=2)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates all n-grams of the given string.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>string</code></strong> :&ensp;<code>str</code></dt>
<dd>The string from which the n-grams will be generated.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of characters in each n-gram. Default is 2.</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>str</code></dt>
<dd>An n-gram of the string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ngrams(string, n=2):
    &#34;&#34;&#34;
    Generates all n-grams of the given string.

    Parameters
    ----------
    string: str
        The string from which the n-grams will be generated.
    n: int, optional
        The number of characters in each n-gram. Default is 2.

    Yields
    ------
    str
        An n-gram of the string.

    &#34;&#34;&#34;
    for i in range(len(string) - n + 1):
        yield string[i:i + n]</code></pre>
</details>
</dd>
<dt id="nama.token_similarity.nmgrams"><code class="name flex">
<span>def <span class="ident">nmgrams</span></span>(<span>string, n=1, m=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates all n-grams of the given string where the n-grams are of length between n and m.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>string</code></strong> :&ensp;<code>str</code></dt>
<dd>The string from which the n-grams will be generated.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The minimum length of the n-grams to be generated. Default is 1.</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The maximum length of the n-grams to be generated. Default is 3.</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>str</code></dt>
<dd>An n-gram of the string where the length is between n and m.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nmgrams(string, n=1, m=3):
    &#34;&#34;&#34;
    Generates all n-grams of the given string where the n-grams are of length between n and m.

    Parameters
    ----------
    string: str
        The string from which the n-grams will be generated.
    n: int, optional
        The minimum length of the n-grams to be generated. Default is 1.
    m: int, optional
        The maximum length of the n-grams to be generated. Default is 3.

    Yields
    ------
    str
        An n-gram of the string where the length is between n and m.

    &#34;&#34;&#34;
    for j in range(n, m + 1):
        for i in range(len(string) - j + 1):
            yield string[i:i + j]</code></pre>
</details>
</dd>
<dt id="nama.token_similarity.words"><code class="name flex">
<span>def <span class="ident">words</span></span>(<span>string)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates all words in the given string.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>string</code></strong> :&ensp;<code>str</code></dt>
<dd>The string from which the words will be extracted.</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>str</code></dt>
<dd>A word extracted from the string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def words(string):
    &#34;&#34;&#34;
    Generates all words in the given string.

    Parameters
    ----------
    string: str
        The string from which the words will be extracted.

    Yields
    ------
    str
        A word extracted from the string.

    &#34;&#34;&#34;
    for m in re.finditer(r&#39;[A-Za-z0-9]+&#39;, string):
        yield m.group(0)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="nama.token_similarity.TokenSimilarity"><code class="flex name class">
<span>class <span class="ident">TokenSimilarity</span></span>
<span>(</span><span>tokenizer=&lt;function TokenSimilarity.&lt;lambda&gt;&gt;, weighting='tf-idf', measure='jaccard', max_block_size=100)</span>
</code></dt>
<dd>
<div class="desc"><p>Configures the token similarity model</p>
<h2 id="parameters">Parameters:</h2>
<p>tokenizer : callable
A function that takes a string an returns an iterable token generator (default: bigrams and trigrams)
weighting : str
One of: None,"tf","idf", or ""tf-idf"
measure : str
Either "jaccard" or "cosine"
max_block_size : int
Maximum size of blocks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TokenSimilarity():
    class TokenSimilarity:
        &#34;&#34;&#34;
        Class that implements a token-based similarity model.
        &#34;&#34;&#34;

    def __init__(
            self,
            tokenizer=lambda s: nmgrams(
                s,
                2,
                3),
            weighting=&#39;tf-idf&#39;,
            measure=&#39;jaccard&#39;,
            max_block_size=100):
        &#34;&#34;&#34;
        Configures the token similarity model

        Parameters:
        -----------
        tokenizer : callable
            A function that takes a string an returns an iterable token generator (default: bigrams and trigrams)
        weighting : str
            One of: None,&#34;tf&#34;,&#34;idf&#34;, or &#34;&#34;tf-idf&#34;
        measure : str
            Either &#34;jaccard&#34; or &#34;cosine&#34;
        max_block_size : int
            Maximum size of blocks.
        &#34;&#34;&#34;

        self.tokenizer = tokenizer
        self.weighting = weighting
        self.measure = measure
        self.max_block_size = max_block_size

        # Assign weighting function
        if callable(weighting):
            self.weight_func = weighting
        elif weighting == &#39;tf&#39;:
            self.weight_func = lambda t, f, d: f
        elif weighting == &#39;idf&#39;:
            self.weight_func = lambda t, f, d: 1 / np.log(1 + d)
        elif weighting == &#39;tf-idf&#39;:
            self.weight_func = lambda t, f, d: f / np.log(1 + d)
        else:
            raise ValueError(&#39;Unknown weighting type&#39;)

        # Assign scoring function
        if callable(measure):
            self.score_func = measure
        elif measure == &#39;cosine&#39;:
            self.score_func = cosine_similarity
        elif measure == &#39;jaccard&#39;:
            self.score_func = jaccard_similarity
        else:
            raise ValueError(&#39;Unknown measure value&#39;)

        # Set threshold to None initially
        self.threshold = None

    def fit(self, matcher, learn_threshold=False,):
        &#34;&#34;&#34;
        Fits the token similarity model on the given matcher data.

        Parameters:
        -----------
        matcher : object
            The matcher object.
        learn_threshold : bool
            If True, learn the threshold.
        &#34;&#34;&#34;
        self.strings = matcher.strings()

        # Loops will run a little faster if we make local references to
        # functions
        tokenizer = self.tokenizer
        weight_func = self.weight_func

        # Tokenize strings
        tokenized = {s: list(tokenizer(s)) for s in self.strings}

        # Count occurrences of tokens
        counts = Counter(t for tokens in tokenized.values() for t in tokens)

        # Convert tokens to sets
        # (drops frequency information, and also makes membership tests faster)
        self.tokenized = {s: set(tokens) for s, tokens in tokenized.items()}

        # Count tokens again, this time tracking the number of strings
        # containing the token. In NLP this is usually referred to as the
        # &#34;document count&#34;
        doc_counts = Counter(t for tokens in tokenized.values()
                             for t in tokens)
        self.doc_counts = doc_counts

        # Build weights
        self.weights = {
            t: weight_func(
                t,
                f,
                doc_counts[t]) for t,
            f in counts.items()}

    def learn_threshold(
            self,
            gold_matcher,
            objective=&#39;F1&#39;,
            grid=np.linspace(
                0.5,
                1,
                100),
            use_counts=False):
        &#34;&#34;&#34;
        Uses train_matcher as a training set to choose the default similarity threshold.

        Parameters:
        -----------
        gold_matcher : object
            Gold matcher data.
        objective : str
            The evaluation objective.
        grid : ndarray
            Grid of values to evaluate.
        use_counts : bool
            If True, use counts.

        Returns:
        -------
        pandas.DataFrame
            The scores data frame.
        &#34;&#34;&#34;
        self_copy = TokenSimilarity(
            tokenizer=self.tokenizer,
            weighting=self.weighting,
            measure=self.measure,
            max_block_size=self.max_block_size)
        self_copy.fit(gold_matcher)

        scores = []
        for t in tqdm(grid):
            pred = self_copy.predict(threshold=t)
            s = score_predicted(pred, gold_matcher, use_counts=use_counts)
            s[&#39;threshold&#39;] = t
            scores.append(s)

        scores_df = pd.DataFrame(scores)

        self.threshold = scores_df[scores_df[objective] ==
                                   scores_df[objective].max()][&#39;threshold&#39;].values[-1]

        return scores_df

    def test(self, test_matcher):
        &#34;&#34;&#34;
        Evaluates the accuracy of the model using test_matcher as the gold-standard test set.

        Note: For a fair test, it may be important to ensure that the test and train matchers have no strings in common.

        Parameters:
        -----------
        test_matcher : object
            The test matcher data.

        Returns:
        -------
        dict
            The scores of the test.
        &#34;&#34;&#34;

        predicted = self.predict(test_matcher.strings())

        scores = scores = score_predicted(predicted, test_matcher)

        return scores

    def predict(self, strings=None, threshold=None):
        &#34;&#34;&#34;
        Uses the similarity model to predict matches between the passed strings.

        Parameters:
        -----------
        strings : list or None, optional
            The list of strings.
        threshold : float or None, optional
            The similarity threshold.

        Returns:
        -------
        dict
            The predicted matches.
        &#34;&#34;&#34;

        if strings:
            self.fit(strings)

        if threshold is None:
            if self.threshold is None:
                raise ValueError(
                    &#39;Must set a threshold value, either by calling .learn_threshold(), or by passing a manually chosen value as an argument&#39;)
            else:
                threshold = self.threshold

        strings = self.strings
        tokenized = self.tokenized
        weights = self.weights
        score_func = self.score_func

        predicted = Matcher(strings)

        # Iterate over unique tokens
        for t, d in self.doc_counts.items():
            # Find all strings that share this token (the &#34;block&#34;)
            if 2 &lt;= d &lt;= self.max_block_size:
                block = [s for s in strings if t in tokenized[s]]

                # Score all pairs in the block
                for s0, s1 in combinations(block, 2):

                    # Can skip pairs that are already in the same group
                    if predicted[s0] != predicted[s1]:

                        # Unite strings with score &gt;= threshold
                        score = score_func(
                            tokenized[s0], tokenized[s1], weights)
                        if score &gt;= threshold:
                            predicted.unite([s0, s1], inplace=True)

        return predicted</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="nama.token_similarity.TokenSimilarity.TokenSimilarity"><code class="name">var <span class="ident">TokenSimilarity</span></code></dt>
<dd>
<div class="desc"><p>Class that implements a token-based similarity model.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="nama.token_similarity.TokenSimilarity.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, matcher, learn_threshold=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits the token similarity model on the given matcher data.</p>
<h2 id="parameters">Parameters:</h2>
<p>matcher : object
The matcher object.
learn_threshold : bool
If True, learn the threshold.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, matcher, learn_threshold=False,):
    &#34;&#34;&#34;
    Fits the token similarity model on the given matcher data.

    Parameters:
    -----------
    matcher : object
        The matcher object.
    learn_threshold : bool
        If True, learn the threshold.
    &#34;&#34;&#34;
    self.strings = matcher.strings()

    # Loops will run a little faster if we make local references to
    # functions
    tokenizer = self.tokenizer
    weight_func = self.weight_func

    # Tokenize strings
    tokenized = {s: list(tokenizer(s)) for s in self.strings}

    # Count occurrences of tokens
    counts = Counter(t for tokens in tokenized.values() for t in tokens)

    # Convert tokens to sets
    # (drops frequency information, and also makes membership tests faster)
    self.tokenized = {s: set(tokens) for s, tokens in tokenized.items()}

    # Count tokens again, this time tracking the number of strings
    # containing the token. In NLP this is usually referred to as the
    # &#34;document count&#34;
    doc_counts = Counter(t for tokens in tokenized.values()
                         for t in tokens)
    self.doc_counts = doc_counts

    # Build weights
    self.weights = {
        t: weight_func(
            t,
            f,
            doc_counts[t]) for t,
        f in counts.items()}</code></pre>
</details>
</dd>
<dt id="nama.token_similarity.TokenSimilarity.learn_threshold"><code class="name flex">
<span>def <span class="ident">learn_threshold</span></span>(<span>self, gold_matcher, objective='F1', grid=array([0.5
, 0.50505051, 0.51010101, 0.51515152, 0.52020202,
0.52525253, 0.53030303, 0.53535354, 0.54040404, 0.54545455,
0.55050505, 0.55555556, 0.56060606, 0.56565657, 0.57070707,
0.57575758, 0.58080808, 0.58585859, 0.59090909, 0.5959596 ,
0.6010101 , 0.60606061, 0.61111111, 0.61616162, 0.62121212,
0.62626263, 0.63131313, 0.63636364, 0.64141414, 0.64646465,
0.65151515, 0.65656566, 0.66161616, 0.66666667, 0.67171717,
0.67676768, 0.68181818, 0.68686869, 0.69191919, 0.6969697 ,
0.7020202 , 0.70707071, 0.71212121, 0.71717172, 0.72222222,
0.72727273, 0.73232323, 0.73737374, 0.74242424, 0.74747475,
0.75252525, 0.75757576, 0.76262626, 0.76767677, 0.77272727,
0.77777778, 0.78282828, 0.78787879, 0.79292929, 0.7979798 ,
0.8030303 , 0.80808081, 0.81313131, 0.81818182, 0.82323232,
0.82828283, 0.83333333, 0.83838384, 0.84343434, 0.84848485,
0.85353535, 0.85858586, 0.86363636, 0.86868687, 0.87373737,
0.87878788, 0.88383838, 0.88888889, 0.89393939, 0.8989899 ,
0.9040404 , 0.90909091, 0.91414141, 0.91919192, 0.92424242,
0.92929293, 0.93434343, 0.93939394, 0.94444444, 0.94949495,
0.95454545, 0.95959596, 0.96464646, 0.96969697, 0.97474747,
0.97979798, 0.98484848, 0.98989899, 0.99494949, 1.
]), use_counts=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Uses train_matcher as a training set to choose the default similarity threshold.</p>
<h2 id="parameters">Parameters:</h2>
<p>gold_matcher : object
Gold matcher data.
objective : str
The evaluation objective.
grid : ndarray
Grid of values to evaluate.
use_counts : bool
If True, use counts.</p>
<h2 id="returns">Returns:</h2>
<p>pandas.DataFrame
The scores data frame.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learn_threshold(
        self,
        gold_matcher,
        objective=&#39;F1&#39;,
        grid=np.linspace(
            0.5,
            1,
            100),
        use_counts=False):
    &#34;&#34;&#34;
    Uses train_matcher as a training set to choose the default similarity threshold.

    Parameters:
    -----------
    gold_matcher : object
        Gold matcher data.
    objective : str
        The evaluation objective.
    grid : ndarray
        Grid of values to evaluate.
    use_counts : bool
        If True, use counts.

    Returns:
    -------
    pandas.DataFrame
        The scores data frame.
    &#34;&#34;&#34;
    self_copy = TokenSimilarity(
        tokenizer=self.tokenizer,
        weighting=self.weighting,
        measure=self.measure,
        max_block_size=self.max_block_size)
    self_copy.fit(gold_matcher)

    scores = []
    for t in tqdm(grid):
        pred = self_copy.predict(threshold=t)
        s = score_predicted(pred, gold_matcher, use_counts=use_counts)
        s[&#39;threshold&#39;] = t
        scores.append(s)

    scores_df = pd.DataFrame(scores)

    self.threshold = scores_df[scores_df[objective] ==
                               scores_df[objective].max()][&#39;threshold&#39;].values[-1]

    return scores_df</code></pre>
</details>
</dd>
<dt id="nama.token_similarity.TokenSimilarity.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, strings=None, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Uses the similarity model to predict matches between the passed strings.</p>
<h2 id="parameters">Parameters:</h2>
<p>strings : list or None, optional
The list of strings.
threshold : float or None, optional
The similarity threshold.</p>
<h2 id="returns">Returns:</h2>
<p>dict
The predicted matches.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, strings=None, threshold=None):
    &#34;&#34;&#34;
    Uses the similarity model to predict matches between the passed strings.

    Parameters:
    -----------
    strings : list or None, optional
        The list of strings.
    threshold : float or None, optional
        The similarity threshold.

    Returns:
    -------
    dict
        The predicted matches.
    &#34;&#34;&#34;

    if strings:
        self.fit(strings)

    if threshold is None:
        if self.threshold is None:
            raise ValueError(
                &#39;Must set a threshold value, either by calling .learn_threshold(), or by passing a manually chosen value as an argument&#39;)
        else:
            threshold = self.threshold

    strings = self.strings
    tokenized = self.tokenized
    weights = self.weights
    score_func = self.score_func

    predicted = Matcher(strings)

    # Iterate over unique tokens
    for t, d in self.doc_counts.items():
        # Find all strings that share this token (the &#34;block&#34;)
        if 2 &lt;= d &lt;= self.max_block_size:
            block = [s for s in strings if t in tokenized[s]]

            # Score all pairs in the block
            for s0, s1 in combinations(block, 2):

                # Can skip pairs that are already in the same group
                if predicted[s0] != predicted[s1]:

                    # Unite strings with score &gt;= threshold
                    score = score_func(
                        tokenized[s0], tokenized[s1], weights)
                    if score &gt;= threshold:
                        predicted.unite([s0, s1], inplace=True)

    return predicted</code></pre>
</details>
</dd>
<dt id="nama.token_similarity.TokenSimilarity.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>self, test_matcher)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the accuracy of the model using test_matcher as the gold-standard test set.</p>
<p>Note: For a fair test, it may be important to ensure that the test and train matchers have no strings in common.</p>
<h2 id="parameters">Parameters:</h2>
<p>test_matcher : object
The test matcher data.</p>
<h2 id="returns">Returns:</h2>
<p>dict
The scores of the test.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test(self, test_matcher):
    &#34;&#34;&#34;
    Evaluates the accuracy of the model using test_matcher as the gold-standard test set.

    Note: For a fair test, it may be important to ensure that the test and train matchers have no strings in common.

    Parameters:
    -----------
    test_matcher : object
        The test matcher data.

    Returns:
    -------
    dict
        The scores of the test.
    &#34;&#34;&#34;

    predicted = self.predict(test_matcher.strings())

    scores = scores = score_predicted(predicted, test_matcher)

    return scores</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="nama" href="index.html">nama</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="nama.token_similarity.cosine_similarity" href="#nama.token_similarity.cosine_similarity">cosine_similarity</a></code></li>
<li><code><a title="nama.token_similarity.jaccard_similarity" href="#nama.token_similarity.jaccard_similarity">jaccard_similarity</a></code></li>
<li><code><a title="nama.token_similarity.ngrams" href="#nama.token_similarity.ngrams">ngrams</a></code></li>
<li><code><a title="nama.token_similarity.nmgrams" href="#nama.token_similarity.nmgrams">nmgrams</a></code></li>
<li><code><a title="nama.token_similarity.words" href="#nama.token_similarity.words">words</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="nama.token_similarity.TokenSimilarity" href="#nama.token_similarity.TokenSimilarity">TokenSimilarity</a></code></h4>
<ul class="">
<li><code><a title="nama.token_similarity.TokenSimilarity.TokenSimilarity" href="#nama.token_similarity.TokenSimilarity.TokenSimilarity">TokenSimilarity</a></code></li>
<li><code><a title="nama.token_similarity.TokenSimilarity.fit" href="#nama.token_similarity.TokenSimilarity.fit">fit</a></code></li>
<li><code><a title="nama.token_similarity.TokenSimilarity.learn_threshold" href="#nama.token_similarity.TokenSimilarity.learn_threshold">learn_threshold</a></code></li>
<li><code><a title="nama.token_similarity.TokenSimilarity.predict" href="#nama.token_similarity.TokenSimilarity.predict">predict</a></code></li>
<li><code><a title="nama.token_similarity.TokenSimilarity.test" href="#nama.token_similarity.TokenSimilarity.test">test</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>